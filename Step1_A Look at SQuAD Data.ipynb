{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SQuAD Data Preprocessing\n",
    "\n",
    "  ### Step1 : SQuAD data structure\n",
    "  \n",
    "    input : List[ entry : DICT ]\n",
    "    entry : DICT[title:str , paragraphs: LIST]\n",
    "    paragraphs : LIST[ paragraph: DICT]\n",
    "    paragraph : DICT[context:str, qas: LIST]\n",
    "    qas: DICT[ qa : DICT]\n",
    "    qa : DICT[id:str , question:str , answers: LIST[DICT]]\n",
    "    answers : LIST[ answer : DICT]\n",
    "    answer : DICT[ answer_start : INT, text:str]\n",
    "\n",
    "![](squad_json.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load squad file  stored as .json file\n",
    "\n",
    "    * Note : \"answer_start\" is char based offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/squad_.json','r',encoding = 'utf-8')as reader:\n",
    "    input_data = json.load(reader)[\"data\"]\n",
    "# Entry\n",
    "for entry in input_data:\n",
    "    # Each entry has \"title\" and \"paragraphs\"\n",
    "    print('Entry \"{}\"\\n'.format(entry['title']))\n",
    "    \n",
    "    for paragraph in entry['paragraphs']:\n",
    "        \n",
    "        # Each paragraph has multiple (question,answer) pairs \n",
    "        for id1 , qa in enumerate(paragraph['qas']):\n",
    "            \n",
    "            print(\"     * question #{} with qa_id : \".format(id1), qa['id'])\n",
    "            print(\"               : \", qa['question'])\n",
    "            \n",
    "            for id2 , answer in enumerate(qa['answers']):\n",
    "            \n",
    "                print(\"     * answer # {} : '{}'. Start position : {}\\n\".format(\n",
    "                                        id2, answer['text'],answer['answer_start']))\n",
    "        \n",
    "        print('\\n     * Context of paragraph \\n{}\\n\\n'.format( paragraph['context'][:500]))\n",
    "    \n",
    "    # look at just one entry\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2. Define a class to store SQuAD samples\n",
    "    \n",
    "    * All positions are word based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                qas_id,\n",
    "                question_text,\n",
    "                doc_tokens,\n",
    "                orig_answer_text=None,\n",
    "                start_position=None,\n",
    "                end_position=None):\n",
    "        \n",
    "        self.qas_id = qas_id\n",
    "        self.question_text = question_text\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.orig_answer_text = orig_answer_text\n",
    "        # Note: The start and end positions stores \n",
    "        #word based indexing positions\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        \n",
    "    def __str__(self):\n",
    "        \n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s=\"\"\n",
    "        s+=\"  - qas_id: {}\".format(self.qas_id)\n",
    "        s+=\"\\n  - question_text: {}\".format(self.question_text)\n",
    "        s+=\"\\n  - answer_text  : {}\".format(self.orig_answer_text)\n",
    "        \n",
    "        if self.start_position:\n",
    "            s += \"\\n  - start_position: {}\".format(self.start_position)\n",
    "        if self.start_position:\n",
    "            s += \"\\n  - end_position: {}\".format(self.end_position)\n",
    "            s += \"\\n  - doc_tokens: \\n\\n     {}\".format(\" \".join(self.doc_tokens))\n",
    "        return s\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Load as a list of SquadExamples\n",
    "\n",
    "  * Conversion from char offset to word offset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training=True\n",
    "entry = input_data[0]\n",
    "paragraph = entry['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 .1. computes the char to word offset map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original text in paragraph\n",
    "paragraph_text = paragraph[\"context\"]\n",
    "\n",
    "doc_tokens = []\n",
    "char_to_word_offset =[]\n",
    "prev_is_whitespace = True\n",
    "\n",
    "# Token Generation \n",
    "for c in paragraph_text:\n",
    "\n",
    "    if c== \" \" or c== \"\\t\" or c ==\"\\r\" or c ==\"\\n\" or ord(c) == 0x202F:\n",
    "        prev_is_whitespace = True\n",
    "    else:\n",
    "        if prev_is_whitespace:\n",
    "            doc_tokens.append(c)\n",
    "        else:\n",
    "            doc_tokens[-1] += c\n",
    "        prev_is_whitespace = False\n",
    "\n",
    "    # make a mapping from char to word\n",
    "    char_to_word_offset.append(len(doc_tokens) -1)\n",
    "\n",
    "print(paragraph_text[:27])\n",
    "print(char_to_word_offset[:27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Extract question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are multiple qas in a paragraph['qas']\n",
    "# Let's look at the first one\n",
    "qa = paragraph['qas'][0]\n",
    "\n",
    "qas_id =qa['id']\n",
    "question_text = qa['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Extract Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data has answers too\n",
    "start_position = None\n",
    "end_position = None\n",
    "orig_answer_text = None\n",
    "\n",
    "if is_training:\n",
    "\n",
    "    # Answer extraction \n",
    "    answer = qa['answers'][0]\n",
    "    orig_answer_text = answer['text']\n",
    "    #char offset to word offset\n",
    "    answer_offset  = answer['answer_start']\n",
    "    answer_length  = len(orig_answer_text)\n",
    "    start_position = char_to_word_offset[answer_offset]\n",
    "    end_position   = char_to_word_offset[answer_offset + answer_length -1]\n",
    "\n",
    "    \n",
    "    # Just minor preprecessing (joined tokens must be same as original text)\n",
    "    actual_text = \" \".join(doc_tokens[start_position:(end_position +1)])\n",
    "    orig_answer_tokens = orig_answer_text.strip()\n",
    "    if not orig_answer_tokens:\n",
    "        original_answer_tokens = []\n",
    "    cleaned_answer_text = \" \".join(orig_answer_tokens.split())\n",
    "    if actual_text.find(cleaned_answer_text) == -1:\n",
    "        print(\"this text is not used\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = SquadExample(\n",
    "     qas_id = qas_id,\n",
    "     question_text = question_text,\n",
    "     doc_tokens = doc_tokens,\n",
    "     orig_answer_text = orig_answer_text,\n",
    "     start_position = start_position,\n",
    "     end_position = end_position)\n",
    "\n",
    "print(example,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
