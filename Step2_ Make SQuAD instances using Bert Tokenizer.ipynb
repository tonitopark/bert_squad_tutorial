{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from squad_example import SquadExample\n",
    "with open('data/squad_examples.pickle', 'rb') as f:\n",
    "     squad_examples=pickle.load(f)\n",
    "example = squad_examples[27]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Making Data for SQuaD Task\n",
    "\n",
    "   * In order to use the squad data for BERT it must be tokenized.\n",
    "   * BERT provides basic BertTokenizer for this task\n",
    "        * Sub-word level tokenization is used.\n",
    "        * Should generate mapping between origianal and sub-word token\n",
    "   * Some preprocessing is done for improving answer span\n",
    "   * To fully utilize data, a sliding window approach is adotped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initial Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "bert_model = 'bert-base-multilingual-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "   bert_model ,do_lower_case=True)\n",
    "# The maximum total input sequence length after WordPiece tokenizationm,\n",
    "max_seq_length = 128\n",
    "#When splitting up a long document into chunks, how much stride to take between chunks\n",
    "doc_stride = 64\n",
    "#The maximum number of tokens for the question.\n",
    "max_query_length = 64 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  A look at BERT Tokenizer\n",
    "\n",
    "\n",
    " *  divides tokens into sub_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, token) in enumerate(example.doc_tokens[15:]):\n",
    "    print(\"Token : \", token)\n",
    "    sub_tokens = tokenizer.tokenize(token)\n",
    "    for sub_token in sub_tokens:\n",
    "        print(\"     Sub-Token : \",sub_token)\n",
    "    if i >= 3:\n",
    "        break  # loo first 4 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3. Mapping between the original token and sub_word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_to_orig_index = []\n",
    "orig_to_tok_index = []\n",
    "\n",
    "# list to store sub-word tokens\n",
    "all_doc_tokens =[]\n",
    "\n",
    "for (i, token) in enumerate(example.doc_tokens[:]):\n",
    "    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "    sub_tokens = tokenizer.tokenize(token)\n",
    "    for sub_token in sub_tokens:\n",
    "        tok_to_orig_index.append(i)\n",
    "        all_doc_tokens.append(sub_token)\n",
    "\n",
    "print('token_to_original map : ',tok_to_orig_index[:40])\n",
    "print('original_to_token_map : ',orig_to_tok_index[:40])\n",
    "print('sub_word tokens : ',all_doc_tokens[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Update answer position to sub_word token index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_start_position = None\n",
    "tok_end_position = None\n",
    "\n",
    "# only for training case\n",
    "print(example.doc_tokens[17])\n",
    "tok_start_position = orig_to_tok_index[example.start_position]\n",
    "\n",
    "# if document is shorter than the given end_position \n",
    "if example.end_position < len(example.doc_tokens) -1:\n",
    "    tok_end_position = orig_to_tok_index[example.end_position+1] -1\n",
    "else:\n",
    "    tok_end_position = len(all_doc_tokens) -1\n",
    "\n",
    "print('start position : token-based : ',example.start_position ,\n",
    "      ' , sub-word token: ',tok_start_position)\n",
    "print('end   : ',tok_end_position)\n",
    "\n",
    "print(all_doc_tokens[21:27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Improving answer span\n",
    "    \n",
    "   * Question: What year was John Smith born?\n",
    "     Context: The leader was John Smith (1895-1943).\n",
    "     Answer: 1895\n",
    "    \n",
    "   - Original whitespace-tokenized answer :  \"(1895-1943).\".     \n",
    "     \n",
    "   - WordPiece tokenization :  \"( 1895 - 1943 ) .\" \n",
    "     \n",
    "      -> Change answer span :  1895.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_answer_text = \" \".join(tokenizer.tokenize(example.orig_answer_text))\n",
    "\n",
    "for new_start in range(tok_start_position, tok_end_position + 1):\n",
    "\n",
    "    for new_end in range(tok_end_position, new_start - 1, -1):\n",
    "\n",
    "        text_span = \" \".join(all_doc_tokens[new_start:(new_end + 1)])\n",
    "\n",
    "        if text_span == tok_answer_text:\n",
    "            tok_start_position = new_start\n",
    "            tok_end_position =  new_end\n",
    "\n",
    "print('start : ',tok_start_position)\n",
    "print('end   : ',tok_end_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Sliding Window Approach \n",
    "   * if document is longer than the maximum sequence length\n",
    "     * take chunks of the up to max_seq_length and repeat sampling\n",
    "       with a stride of 'doc_stride'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "_DocSpan = collections.namedtuple(\"DocSpan\",[\"start\",\"length\"])\n",
    "max_seq_length = 128\n",
    "max_query_length = 64\n",
    "\n",
    "# 1. tokenize question text\n",
    "query_tokens = tokenizer.tokenize(example.question_text)\n",
    "\n",
    "# truncate query tokens if larger than max_length (64)\n",
    "if len(query_tokens) > max_query_length:\n",
    "    query_tokens = query_tokens[0:max_query_length]\n",
    "\n",
    "# Account for question tokens & [CLS] [SEP] [SEP]\n",
    "max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "start_offset=0\n",
    "doc_spans = []\n",
    "start_offset = 0\n",
    "\n",
    "while start_offset < len(all_doc_tokens):\n",
    "\n",
    "    length = len(all_doc_tokens) - start_offset\n",
    "    if length > max_tokens_for_doc:\n",
    "        length = max_tokens_for_doc\n",
    "    doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "    if start_offset + length == len(all_doc_tokens):\n",
    "        break\n",
    "    start_offset += min(length, doc_stride)\n",
    "\n",
    "print('length of original document : ' , len(all_doc_tokens))\n",
    "print(\"doc_stride\", doc_stride)\n",
    "print('Document Spans : ',doc_spans)\n",
    "print('\\nFirst_span \\n',all_doc_tokens[doc_spans[0].start:doc_spans[0].start + doc_spans[0].length])\n",
    "print('\\nSecond_span \\n',all_doc_tokens[doc_spans[1].start:doc_spans[1].start + doc_spans[1].length])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Check if token is  max context\n",
    "   * For evaluation of performance the model prefer max span contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "    #  Document: the man went to the store and bought a gallon of milk\n",
    "    #  Span A: the man went to the\n",
    "    #  Span B: to the store and bought\n",
    "    #  Span C: and bought a gallon of\n",
    "    #  ...\n",
    "    # \"maximum context\"\n",
    "    #    =  minimum(  left context,   right context )\n",
    "    \n",
    "\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span.start + doc_span.length - 1\n",
    "        if position < doc_span.start:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span.start\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index\n",
    "\n",
    "\n",
    "# Check if tokens is max context\n",
    "\n",
    "doc_span = doc_spans[0]\n",
    "tokens =[]\n",
    "token_is_max_context ={}\n",
    "# Document tokens\n",
    "for i in range(doc_span.length):\n",
    "\n",
    "    split_token_index = doc_span.start + i\n",
    "\n",
    "    is_max_context = _check_is_max_context(doc_spans, 0,\n",
    "                                         split_token_index)\n",
    "\n",
    "    token_is_max_context[len(tokens)] = is_max_context\n",
    "\n",
    "    # sliding window tokens\n",
    "    tokens.append(all_doc_tokens[split_token_index])\n",
    "\n",
    "print(tokens)\n",
    "print(token_is_max_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make instance from each Document Span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * Inputs:\n",
    "        * input_ids:\n",
    "            - a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary\n",
    "        * token_type_ids:\n",
    "            - a torch.LongTensor of shape [batch_size, sequence_length] with the values in [0, 1]. Type 0 corresponds to a \"sentence A\" and type 1 corresponds to a \"sentence B\" token\n",
    "        * attention_mask : \n",
    "            - a torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1]. only positions with value 1's are attended in the sentences.\n",
    "        * start_positions : \n",
    "            - position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n",
    "        * end_positions: \n",
    "            - position of the last token for the labeled span: torch.LongTensor of shape [batch_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "doc_span_index =0\n",
    "doc_span = doc_spans[0]\n",
    "\n",
    "print(doc_span_index)\n",
    "tokens =[]\n",
    "token_to_orig_map ={}\n",
    "token_is_max_context ={}\n",
    "segment_ids =[]\n",
    "\n",
    "# initial symbol\n",
    "tokens.append(\"[CLS]\")\n",
    "segment_ids.append(0)\n",
    "\n",
    "# Question tokens\n",
    "for token in query_tokens:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(0)\n",
    "\n",
    "# separator token\n",
    "tokens.append(\"[SEP]\")\n",
    "segment_ids.append(0)\n",
    "print(doc_span_index)\n",
    "\n",
    "# Document tokens\n",
    "for i in range(doc_span.length):\n",
    "    split_token_index = doc_span.start + i\n",
    "\n",
    "    # should make another mapping because the text is sliding window part\n",
    "    token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "    is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                          split_token_index)\n",
    "    token_is_max_context[len(tokens)] = is_max_context\n",
    "\n",
    "    # sliding window (doc_span) tokens\n",
    "    tokens.append(all_doc_tokens[split_token_index])\n",
    "    segment_ids.append(1)\n",
    "\n",
    "\n",
    "#Separator token\n",
    "tokens.append(\"[SEP]\")\n",
    "segment_ids.append(1)\n",
    "print(tokens)\n",
    "print(token_is_max_context)\n",
    "print(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Mask \n",
    "# real tokens : 1  , padding tokens : 0\n",
    "input_mask =[1]*len(input_ids)\n",
    "\n",
    "#Zero-pad up to the sequence length\n",
    "while len(input_ids)< max_seq_length:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    segment_ids.append(0)\n",
    "\n",
    "assert len(input_ids) == max_seq_length\n",
    "assert len(input_mask) == max_seq_length\n",
    "assert len(segment_ids) == max_seq_length\n",
    "\n",
    "print(doc_span_index)\n",
    "\n",
    "start_position = None\n",
    "end_position = None\n",
    "\n",
    "# When training only\n",
    "\n",
    "# for training if document chunk does not contain an annotation \n",
    "# we throw it out, since there is nothing to predict\n",
    "doc_start = doc_span.start\n",
    "doc_end   = doc_span.start + doc_span.length -1\n",
    "print(' ex_st ',example.start_position,' ex_ed ',\n",
    "      example.end_position,' doc_st ',doc_start,' doc_ed ',doc_end)\n",
    "if (example.start_position < doc_start or\n",
    "       example.end_position < doc_start or\n",
    "       example.start_position > doc_end or \n",
    "       example.end_position > doc_end):\n",
    "    print(\"answer is not in the document\")\n",
    "\n",
    "# [CLS] + question tokens + [SEP]\n",
    "doc_offset = len(query_tokens) + 2\n",
    "#               start position new_token   window_position \n",
    "start_position = tok_start_position         - doc_start    + doc_offset\n",
    "end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "example_index = 1\n",
    "unique_id = example.qas_id\n",
    "print(doc_span_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"\n",
    "    A single set of rfeatures of data\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                unique_id,\n",
    "                example_index,\n",
    "                doc_span_index,\n",
    "                tokens,\n",
    "                token_to_orig_map,\n",
    "                token_is_max_context,\n",
    "                input_ids,\n",
    "                input_mask,\n",
    "                segment_ids,\n",
    "                start_position=None,\n",
    "                end_position=None):\n",
    "        self.unique_id = unique_id\n",
    "        self.example_index = example_index\n",
    "        self.doc_span_index = doc_span_index\n",
    "        self.tokens = tokens\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "        self.token_is_max_context = token_is_max_context\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        \n",
    "    def __str__(self):\n",
    "        \n",
    "        return self.__repr__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        s = \"\\n\"\n",
    "        s +=\"unique_id: {} \\n\".format(unique_id)\n",
    "        s +=\"example_index: {} \\n\".format(example_index)\n",
    "        s +=\"doc_span_index: {} \\n\".format(doc_span_index)\n",
    "        s +=\"tokens: {} \\n\".format(\" \".join(tokens))\n",
    "        s +=\"tokens_to_origin_map: {} \\n\".format(\" \".join([\n",
    "            \"{}:{}\".format(x,y) for (x,y) in token_to_orig_map.items()]))\n",
    "        s +=\"token_is_max_content: {} \\n\".format(\" \".join([\n",
    "              \"{}:{}\".format(x,y) for (x,y) in token_is_max_context.items()]))\n",
    "        s +=\"input_ids: {} \\n\".format( \" \".join([str(x) for x in input_ids]))\n",
    "        s +=\"input_mask: {}\\n\".format( \" \".join([str(x) for x in input_mask]))\n",
    "        s +=\"segment_ids: {}\\n\".format(\" \".join([str(x) for x in segment_ids]))\n",
    "        # only when training \n",
    "        s += \"answer_text : {}\\n\".format(\" \".join(tokens[start_position:(end_position +1)]))\n",
    "        s += \"start_position: {}\\n\".format(start_position)\n",
    "        s +=\"end_position : {}\\n \".format(end_position)\n",
    "        return s\n",
    "\n",
    "\n",
    "input = InputFeatures(\n",
    "        unique_id = unique_id,\n",
    "        example_index = example_index,\n",
    "        doc_span_index = doc_span_index,\n",
    "        tokens = tokens,\n",
    "        token_to_orig_map=token_to_orig_map,\n",
    "        token_is_max_context=token_is_max_context,\n",
    "        input_ids = input_ids,\n",
    "        input_mask = input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        start_position = start_position,\n",
    "        end_position = end_position)\n",
    "\n",
    "print(input)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "global_step = 0\n",
    "cached_train_features_file = 'data/train-v1.1.json_{}_{}_{}_{}'.format(\n",
    "    list(filter(None,bert_model.split('/'))).pop(),str(max_seq_length),\n",
    "    str(doc_stride),str(max_query_length))\n",
    "\n",
    "with open(cached_train_features_file,'wb') as writer:\n",
    "    #train_features = features\n",
    "    pickle.dump(train_features,writer)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
