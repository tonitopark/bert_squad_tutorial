{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  PyTorch implementation of BERT\n",
    "\n",
    " * pytorch-pretrained-BERT\n",
    " - [Link](https://github.com/huggingface/pytorch-pretrained-BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "# Bert Config\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "config = BertConfig(vocab_size_or_config_json_file=32000, \n",
    "                    hidden_size=768,\n",
    "                    num_hidden_layers=12, \n",
    "                    num_attention_heads=12, \n",
    "                    intermediate_size=3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Model\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "bert = BertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extending BERT for SQuAD task\n",
    "\n",
    "    * see class modeling.BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 define a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1019,  0.0607, -1.8293,  ...,  0.1973, -0.5558, -0.2524],\n",
      "         [-1.8015,  0.0998, -1.0263,  ..., -0.8126,  0.5128,  0.6052],\n",
      "         [-0.3116, -0.6070, -0.9758,  ..., -0.4103, -0.7104, -1.7373]],\n",
      "\n",
      "        [[-0.8627,  1.5372, -2.1627,  ..., -0.0184, -1.0327, -0.2132],\n",
      "         [ 0.3777,  1.8563, -3.0047,  ..., -1.1234, -0.6937,  0.6427],\n",
      "         [-1.2452,  0.2234, -1.0969,  ..., -1.2471, -0.4385, -0.7992]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 3, 768])\n",
      "tensor([[[-0.0759, -0.0346],\n",
      "         [-0.6210,  0.4544],\n",
      "         [-0.4926,  0.0212]],\n",
      "\n",
      "        [[ 0.3300, -0.2095],\n",
      "         [-0.5700, -0.1374],\n",
      "         [-0.3750,  0.7426]]], grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Define new fully connected layer with two ouputs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# test how it works\n",
    "# inputs\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "#outputs\n",
    "qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "last_encoding_layer, _ = bert(input_ids, token_type_ids, input_mask, output_all_encoded_layers=False)\n",
    "logits = qa_outputs(last_encoding_layer)\n",
    "\n",
    "print(last_encoding_layer)\n",
    "print(last_encoding_layer.size())\n",
    "print(logits)\n",
    "print(logits.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0759, -0.6210, -0.4926],\n",
      "        [ 0.3300, -0.5700, -0.3750]], grad_fn=<SqueezeBackward1>) \n",
      " tensor([[-0.0346,  0.4544,  0.0212],\n",
      "        [-0.2095, -0.1374,  0.7426]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "start_logits, end_logits = logits.split(1, dim=-1)\n",
    "start_logits = start_logits.squeeze(-1)\n",
    "end_logits = end_logits.squeeze(-1)\n",
    "print(start_logits,'\\n',end_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Defining loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4467, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9188, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1827, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# loss definition\n",
    "\n",
    "start_positions=torch.LongTensor([1,1])\n",
    "end_positions=torch.LongTensor([2,2])\n",
    "\n",
    "start_loss = nn.functional.cross_entropy(start_logits, start_positions)\n",
    "end_loss = nn.functional.cross_entropy(end_logits, end_positions)\n",
    "\n",
    "# define the loss as the avarage loss of start and end loss\n",
    "total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "print(start_loss)\n",
    "print(end_loss)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine-Tuning Bert for SQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Make DataLoader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total batch size for training\n",
    "train_batch_size = 6\n",
    "num_train_epochs = 1.0\n",
    "learning_rate=0.1\n",
    "seed = 42\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of input features loaded :  348\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from squad_example import InputFeatures\n",
    "\n",
    "global_step = 0\n",
    "cached_train_features_file = \\\n",
    "'data/tranin-v1.1_sample.json_bert-base-multilingual-cased_128_64_64'\n",
    "\n",
    "# Load cached features\n",
    "with open(cached_train_features_file, \"rb\") as reader:\n",
    "    train_features = pickle.load(reader)\n",
    "print(\"total number of input features loaded : \", len(train_features))\n",
    "\n",
    "# Define Input Tensors\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features],\n",
    "                            dtype = torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features],\n",
    "                             dtype = torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features],\n",
    "                              dtype = torch.long)\n",
    "all_start_positions = torch.tensor([f.start_position for f in train_features],\n",
    "                                  dtype = torch.long)\n",
    "all_end_positions = torch.tensor([f.end_position for f in train_features],\n",
    "                               dtype = torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,\\\n",
    "                              DataLoader, \\\n",
    "                              RandomSampler\n",
    "\n",
    "train_data = TensorDataset(all_input_ids,\n",
    "                           all_input_mask,\n",
    "                           all_segment_ids,\n",
    "                           all_start_positions,\n",
    "                           all_end_positions)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_data,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size = train_batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train steps :  58\n"
     ]
    }
   ],
   "source": [
    "from squad_example import InputFeatures\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization\n",
    "max_seq_length = 512\n",
    "#When splitting up a long document into chunks, how much stride to take between chunks\n",
    "doc_stride = 128\n",
    "#The maximum number of tokens for the question.\n",
    "max_query_length = 64 \n",
    "\n",
    "output_dir = '/tmp/squad'\n",
    "\n",
    "num_train_steps = int(len(train_features) \n",
    "                      / train_batch_size \n",
    "                      * num_train_epochs)\n",
    "\n",
    "print(\"Number of train steps : \", num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "bert_model = 'bert-base-multilingual-cased'\n",
    "model = BertForQuestionAnswering.from_pretrained(\n",
    "            bert_model,\n",
    "            cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(-1))\n",
    "\n",
    "model.to(\"cuda\")\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "#prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "#hack to remove \"pooler\"  which is not used\n",
    "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "no_decay =['bias','LayerNorm.bias','LayerNorm.weight']\n",
    "optimizer_grouped_parameters =[\n",
    "    {'params':[ p for n , p in param_optimizer \n",
    "               if not any(nd in n for nd in no_decay)],'weight_decay':0.01},\n",
    "    {'params':[p for n, p in param_optimizer\n",
    "              if any(nd in n for nd in no_decay)],'weigth_decay':0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                    lr=5e-5,\n",
    "                    warmup=0.1,\n",
    "                    t_total = num_train_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/58 [00:00<?, ?it/s]\u001b[A/home/tony/PycharmProjectsSSH/bert_squad_tutorial/venv/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 350.25 MiB (GPU 0; 11.91 GiB total capacity; 2.70 GiB already allocated; 324.81 MiB free; 12.93 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-64bb36ae11a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjectsSSH/bert_squad_tutorial/venv/lib/python3.6/site-packages/pytorch_pretrained_bert/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mupdate_with_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduled\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mupdate_with_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 350.25 MiB (GPU 0; 11.91 GiB total capacity; 2.70 GiB already allocated; 324.81 MiB free; 12.93 MiB cached)"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "    for step, batch in enumerate(tqdm(train_dataloader,desc=\"Iteration\")):\n",
    "        \n",
    "        batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "        \n",
    "        input_ids,\\\n",
    "        input_mask,\\\n",
    "        segment_ids,\\\n",
    "        start_positions,\\\n",
    "        end_positions=batch\n",
    "        \n",
    "        loss = model(input_ids,\n",
    "                     segment_ids,\n",
    "                     input_mask,\n",
    "                     start_positions,\n",
    "                     end_positions)\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step +=1\n",
    "\n",
    "model_to_save = model.module if hasatt(model,'module') else model\n",
    "output_model_file = os.path.join(output_dir,\"pytorch_model.bin\")\n",
    "torch.save(model_to_save.state_dict(),output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
