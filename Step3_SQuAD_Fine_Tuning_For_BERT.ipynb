{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  PyTorch implementation of BERT\n",
    "\n",
    " * pytorch-pretrained-BERT\n",
    " - [Link](https://github.com/huggingface/pytorch-pretrained-BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Config\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "config = BertConfig(vocab_size_or_config_json_file=32000, \n",
    "                    hidden_size=768,\n",
    "                    num_hidden_layers=12, \n",
    "                    num_attention_heads=12, \n",
    "                    intermediate_size=3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Model\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "bert = BertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extending BERT for SQuAD task\n",
    "\n",
    "    * see class modeling.BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 define a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new fully connected layer with two ouputs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# test how it works\n",
    "# inputs\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "#outputs\n",
    "qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "last_encoding_layer, _ = bert(input_ids, token_type_ids, input_mask, output_all_encoded_layers=False)\n",
    "logits = qa_outputs(last_encoding_layer)\n",
    "\n",
    "print(last_encoding_layer)\n",
    "print(last_encoding_layer.size())\n",
    "print(logits)\n",
    "print(logits.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits, end_logits = logits.split(1, dim=-1)\n",
    "start_logits = start_logits.squeeze(-1)\n",
    "end_logits = end_logits.squeeze(-1)\n",
    "print(start_logits,'\\n',end_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Defining loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss definition\n",
    "\n",
    "start_positions=torch.LongTensor([1,1])\n",
    "end_positions=torch.LongTensor([2,2])\n",
    "\n",
    "start_loss = nn.functional.cross_entropy(start_logits, start_positions)\n",
    "end_loss = nn.functional.cross_entropy(end_logits, end_positions)\n",
    "\n",
    "# define the loss as the avarage loss of start and end loss\n",
    "total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "print(start_loss)\n",
    "print(end_loss)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine-Tuning Bert for SQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Make DataLoader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total batch size for training\n",
    "train_batch_size = 6\n",
    "num_train_epochs = 1.0\n",
    "learning_rate=0.1\n",
    "seed = 42\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from squad_example import InputFeatures\n",
    "\n",
    "global_step = 0\n",
    "cached_train_features_file = \\\n",
    "'data/tranin-v1.1_sample.json_bert-base-multilingual-cased_128_64_64'\n",
    "\n",
    "# Load cached features\n",
    "with open(cached_train_features_file, \"rb\") as reader:\n",
    "    train_features = pickle.load(reader)\n",
    "print(\"total number of input features loaded : \", len(train_features))\n",
    "\n",
    "# Define Input Tensors\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features],\n",
    "                            dtype = torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features],\n",
    "                             dtype = torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features],\n",
    "                              dtype = torch.long)\n",
    "all_start_positions = torch.tensor([f.start_position for f in train_features],\n",
    "                                  dtype = torch.long)\n",
    "all_end_positions = torch.tensor([f.end_position for f in train_features],\n",
    "                               dtype = torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,\\\n",
    "                              DataLoader, \\\n",
    "                              RandomSampler\n",
    "\n",
    "train_data = TensorDataset(all_input_ids,\n",
    "                           all_input_mask,\n",
    "                           all_segment_ids,\n",
    "                           all_start_positions,\n",
    "                           all_end_positions)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_data,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size = train_batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from squad_example import InputFeatures\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization\n",
    "max_seq_length = 512\n",
    "#When splitting up a long document into chunks, how much stride to take between chunks\n",
    "doc_stride = 128\n",
    "#The maximum number of tokens for the question.\n",
    "max_query_length = 64 \n",
    "\n",
    "output_dir = '/tmp/squad'\n",
    "\n",
    "num_train_steps = int(len(train_features) \n",
    "                      / train_batch_size \n",
    "                      * num_train_epochs)\n",
    "\n",
    "print(\"Number of train steps : \", num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "bert_model = 'bert-base-multilingual-cased'\n",
    "model = BertForQuestionAnswering.from_pretrained(\n",
    "            bert_model,\n",
    "            cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(-1))\n",
    "\n",
    "model.to(\"cuda\")\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "#prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "#hack to remove \"pooler\"  which is not used\n",
    "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "no_decay =['bias','LayerNorm.bias','LayerNorm.weight']\n",
    "optimizer_grouped_parameters =[\n",
    "    {'params':[ p for n , p in param_optimizer \n",
    "               if not any(nd in n for nd in no_decay)],'weight_decay':0.01},\n",
    "    {'params':[p for n, p in param_optimizer\n",
    "              if any(nd in n for nd in no_decay)],'weigth_decay':0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                    lr=5e-5,\n",
    "                    warmup=0.1,\n",
    "                    t_total = num_train_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "    for step, batch in enumerate(tqdm(train_dataloader,desc=\"Iteration\")):\n",
    "        \n",
    "        batch = tuple(t.to(\"cuda\") for t in batch)\n",
    "        \n",
    "        input_ids,\\\n",
    "        input_mask,\\\n",
    "        segment_ids,\\\n",
    "        start_positions,\\\n",
    "        end_positions=batch\n",
    "        \n",
    "        loss = model(input_ids,\n",
    "                     segment_ids,\n",
    "                     input_mask,\n",
    "                     start_positions,\n",
    "                     end_positions)\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step +=1\n",
    "\n",
    "model_to_save = model.module if hasatt(model,'module') else model\n",
    "output_model_file = os.path.join(output_dir,\"pytorch_model.bin\")\n",
    "torch.save(model_to_save.state_dict(),output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
